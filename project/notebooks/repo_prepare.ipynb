{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89302d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7534d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"project\"\n",
    "ROOT = Path(PROJECT_NAME)\n",
    "\n",
    "# Define the directory list\n",
    "DIRS = [\n",
    "    ROOT / \"data\" / \"raw\",\n",
    "    ROOT / \"data\" / \"processed\",\n",
    "    ROOT / \"docs\",\n",
    "    ROOT / \"models\",\n",
    "    ROOT / \"notebooks\",\n",
    "    ROOT / \"reports\" / \"figures\",\n",
    "    ROOT / \"src\",\n",
    "    ROOT / \"scripts\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe53380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories ensured: [PosixPath('project/data/raw'), PosixPath('project/data/processed'), PosixPath('project/docs'), PosixPath('project/models'), PosixPath('project/notebooks'), PosixPath('project/reports/figures'), PosixPath('project/src'), PosixPath('project/scripts')]\n"
     ]
    }
   ],
   "source": [
    "# Create the root directory\n",
    "ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Create all subdirectories\n",
    "for d in DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    # Create a .gitkeep file in each directory\n",
    "    (d / \".gitkeep\").touch()\n",
    "\n",
    "print(\"Directories ensured:\", DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b3cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/src/config.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "config_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_environment() -> None:\n",
    "    load_dotenv()\n",
    "\n",
    "def get_api_key(name: str = 'ALPHAVANTAGE_API_KEY') -> str | None:\n",
    "    return os.getenv(name)\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"src/config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(config_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'src/config.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0df52fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/src/storage.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "storage_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_format(path: str | Path) -> str:\n",
    "    ext = str(path).lower().rsplit('.', 1)[-1]\n",
    "    if ext in ('csv', 'parquet'):\n",
    "        return ext\n",
    "    raise ValueError(f'Unsupported file extension: {ext}')\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: str | Path) -> Path:\n",
    "    path = Path(path)\n",
    "    fmt = detect_format(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == 'csv':\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    elif fmt == 'parquet':\n",
    "        try:\n",
    "            df.to_parquet(path, index=False)\n",
    "        except Exception:\n",
    "            fallback = path.with_suffix('.csv')\n",
    "            df.to_csv(fallback, index=False)\n",
    "            return fallback\n",
    "        return path\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported format: {fmt}')\n",
    "\n",
    "def read_df(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    fmt = detect_format(path)\n",
    "    if fmt == 'csv':\n",
    "        df = pd.read_csv(path)\n",
    "        if 'date' in df.columns:\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return df\n",
    "    elif fmt == 'parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported format: {fmt}')\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"src/storage.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(storage_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'src/storage.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9429f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/src/cleaning.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "cleaning_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "def fill_missing_median(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns and pd.api.types.is_numeric_dtype(out[c]):\n",
    "            med = out[c].median()\n",
    "            out[c] = out[c].fillna(med)\n",
    "    return out\n",
    "\n",
    "def drop_missing(df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # drop columns above threshold missing\n",
    "    col_missing = out.isna().mean()\n",
    "    to_drop = [c for c, r in col_missing.items() if r > threshold]\n",
    "    if to_drop:\n",
    "        out = out.drop(columns=to_drop)\n",
    "    # drop remaining rows with any missing\n",
    "    out = out.dropna(axis=0, how='any')\n",
    "    return out\n",
    "\n",
    "def normalize_data(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns and pd.api.types.is_numeric_dtype(out[c]):\n",
    "            mu = out[c].mean()\n",
    "            sigma = out[c].std(ddof=0)\n",
    "            if sigma and not np.isnan(sigma) and sigma != 0:\n",
    "                out[c] = (out[c] - mu) / sigma\n",
    "    return out\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"src/cleaning.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaning_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'src/cleaning.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c797c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/src/outliers.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "outliers_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Iterable, Dict, Optional, Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OutlierMethod = Literal[\"iqr\", \"zscore\"]\n",
    "HandleMode = Literal[\"flag\", \"remove\", \"winsorize\", \"none\"]\n",
    "\n",
    "__all__ = [\n",
    "    \"detect_outliers_iqr\",\n",
    "    \"detect_outliers_zscore\",\n",
    "    \"winsorize_series\",\n",
    "    \"flag_outliers_df\",\n",
    "    \"remove_outliers_df\",\n",
    "    \"winsorize_df\",\n",
    "]\n",
    "\n",
    "def detect_outliers_iqr(series: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - k * iqr\n",
    "    upper = q3 + k * iqr\n",
    "    mask = (series < lower) | (series > upper)\n",
    "    return mask.fillna(False)\n",
    "\n",
    "def detect_outliers_zscore(series: pd.Series, threshold: float = 3.0) -> pd.Series:\n",
    "    mu = series.mean(skipna=True)\n",
    "    sigma = series.std(ddof=0, skipna=True)\n",
    "    if sigma == 0 or np.isnan(sigma):\n",
    "        return pd.Series(False, index=series.index)\n",
    "    z = (series - mu) / sigma\n",
    "    mask = z.abs() > threshold\n",
    "    return mask.fillna(False)\n",
    "\n",
    "def winsorize_series(series: pd.Series, lower: float = 0.05, upper: float = 0.95) -> pd.Series:\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    lo = series.quantile(lower)\n",
    "    hi = series.quantile(upper)\n",
    "    return series.clip(lower=lo, upper=hi)\n",
    "\n",
    "def flag_outliers_df(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Optional[Iterable[str]] = None,\n",
    "    method: OutlierMethod = \"iqr\",\n",
    "    method_params: Optional[Dict] = None,\n",
    "    flag_suffix: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    method_params = method_params or {}\n",
    "    out = df.copy()\n",
    "    for col in columns:\n",
    "        if not pd.api.types.is_numeric_dtype(out[col]):\n",
    "            continue\n",
    "        if method == \"iqr\":\n",
    "            mask = detect_outliers_iqr(out[col], **method_params)\n",
    "            suffix = flag_suffix or \"outlier_iqr\"\n",
    "        elif method == \"zscore\":\n",
    "            mask = detect_outliers_zscore(out[col], **method_params)\n",
    "            suffix = flag_suffix or \"outlier_z\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        out[f\"{col}_{suffix}\"] = mask\n",
    "    return out\n",
    "\n",
    "def remove_outliers_df(\n",
    "    df: pd.DataFrame,\n",
    "    flag_columns: Optional[Iterable[str]] = None,\n",
    "    how: Literal[\"any\", \"all\"] = \"any\",\n",
    ") -> pd.DataFrame:\n",
    "    if flag_columns is None:\n",
    "        flag_columns = [c for c in df.columns if \"outlier\" in c]\n",
    "    if not flag_columns:\n",
    "        return df.copy()\n",
    "    mask = df[flag_columns].any(axis=1) if how == \"any\" else df[flag_columns].all(axis=1)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def winsorize_df(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Optional[Iterable[str]] = None,\n",
    "    lower: float = 0.05,\n",
    "    upper: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    out = df.copy()\n",
    "    for col in columns:\n",
    "        if pd.api.types.is_numeric_dtype(out[col]):\n",
    "            out[col] = winsorize_series(out[col], lower=lower, upper=upper)\n",
    "    return out\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"src/outliers.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(outliers_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'src/outliers.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d07f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/scripts/eda.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "eda_py = dedent(\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def run_eda(df):\n",
    "    sns.set(context='talk', style='whitegrid')\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "    print(df.info(), df.isna().sum())\n",
    "    desc = df[['age','income','transactions','spend']].describe().T\n",
    "    desc['skew'] = [skew(df[c].dropna()) for c in desc.index]\n",
    "    desc['kurtosis'] = [kurtosis(df[c].dropna()) for c in desc.index]\n",
    "    print(desc)\n",
    "    \n",
    "    sns.histplot(df['income'], kde=True)\n",
    "    plt.title('Income Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(x=df['transactions'])\n",
    "    plt.title('Transactions (Outliers)')\n",
    "    plt.show()\n",
    "\n",
    "    sns.histplot(df['spend'], kde=True)\n",
    "    plt.title('Spend Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    sns.scatterplot(data=df, x='income', y='spend')\n",
    "    plt.title('Spend vs Income')\n",
    "    plt.show()\n",
    "\n",
    "    sns.countplot(data=df, x='region')\n",
    "    plt.title('Count by Region')\n",
    "    plt.show()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"scripts/eda.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(eda_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'scripts/eda.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00870a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/scripts/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "feature_engineering_py = dedent(\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "def create_features(df):\n",
    "    df['daily_return'] = df['close'].pct_change()\n",
    "    df['rolling_avg_5d_close'] = df['close'].rolling(window=5).mean()\n",
    "    df['rolling_vol_5d'] = df['daily_return'].rolling(window=5).std()\n",
    "    return df\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"scripts/feature_engineering.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(feature_engineering_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'scripts/feature_engineering.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ac70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/scripts/modeling.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "modeling_py = dedent(\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_regression_model(df):\n",
    "    df['return'] = df['close'].pct_change()\n",
    "    y = df['return'].shift(-1)\n",
    "    features = ['open', 'high', 'low', 'close', 'volume']\n",
    "    X = df[features]\n",
    "    combined = pd.concat([y, X], axis=1)\n",
    "    combined.dropna(inplace=True)\n",
    "    y = combined['return']\n",
    "    X = combined[features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    lr = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Baseline (predicting returns)   R²={r2:.4f}  RMSE={rmse:.6f}')\n",
    "    return lr, X_test, y_test\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"scripts/modeling.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(modeling_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'scripts/modeling.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8fc97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/scripts/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "evaluation_py = dedent(\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def mean_impute(a: np.ndarray) -> np.ndarray:\n",
    "    m = np.nanmean(a)\n",
    "    out = a.copy()\n",
    "    out[np.isnan(out)] = m\n",
    "    return out\n",
    "\n",
    "def evaluate_model(df):\n",
    "    X_raw = df['x_feature'].values\n",
    "    y = df['y_target'].values\n",
    "    X_base = mean_impute(X_raw)\n",
    "    model = LinearRegression().fit(X_base.reshape(-1,1), y)\n",
    "    y_hat = model.predict(X_base.reshape(-1,1))\n",
    "    df['x_imputed'] = X_base\n",
    "    base_mae = mean_absolute_error(y, y_hat)\n",
    "    print(f\"Base MAE: {base_mae}\")\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"scripts/evaluation.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(evaluation_py)\n",
    "    \n",
    "print(f\"Written {ROOT / 'scripts/evaluation.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21db11a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/scripts/reporting.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "reporting_py = dedent(\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_report(df):\n",
    "    img_dir = Path('../deliverables/images')\n",
    "    img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.scatterplot(data=df, x='volatility', y='return', hue='scenario', s=80)\n",
    "    plt.title('Risk–Return by Scenario')\n",
    "    plt.xlabel('Volatility')\n",
    "    plt.ylabel('Return')\n",
    "    plt.savefig(img_dir / 'risk_return.png', dpi=300)\n",
    "    plt.show()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"scripts/reporting.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reporting_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'scripts/reporting.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe4aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/app.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "productization_py = dedent(\"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the model\n",
    "with open('model/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    features = data.get('features', None)\n",
    "    if features is None:\n",
    "        return jsonify({'error': 'No features provided'}), 400\n",
    "    prediction = model.predict([features])\n",
    "    return jsonify({'prediction': prediction[0]})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(productization_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'app.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de7a7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/main.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "main_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.config import load_environment\n",
    "from src.storage import read_df, write_df\n",
    "from scripts import eda, feature_engineering, modeling, evaluation, reporting\n",
    "\n",
    "def main():\n",
    "    load_environment()\n",
    "    \n",
    "    # Example usage of imported modules\n",
    "    DATA_PATH = Path('data/raw/api_aapl.csv')\n",
    "    if DATA_PATH.exists():\n",
    "        df = read_df(DATA_PATH)\n",
    "        df_featured = feature_engineering.create_features(df.copy())\n",
    "        model, X_test, y_test = modeling.train_regression_model(df_featured.copy())\n",
    "        \n",
    "        # This part needs a DataFrame with 'x_feature' and 'y_target' for evaluation.\n",
    "        # Since the provided notebooks have different data, we'll note this.\n",
    "        # evaluation.evaluate_model(some_other_df) \n",
    "        \n",
    "        # This part needs a DataFrame with scenario analysis results for reporting.\n",
    "        # reporting.generate_report(scenario_df)\n",
    "    else:\n",
    "        print(f\"{DATA_PATH} not found. Run the acquisition script first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"main.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(main_py)\n",
    "\n",
    "print(f\"Written {ROOT / 'main.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b89ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written project/README.md\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "readme = dedent(\"\"\"\n",
    "# Financial Engineering Project\n",
    "\n",
    "## Overview\n",
    "This project provides a comprehensive, end-to-end pipeline for financial analysis, including data acquisition, preprocessing, feature engineering, modeling, evaluation, and productization.\n",
    "\n",
    "## Project Structure\n",
    "- `data/`: Raw, interim, and processed datasets\n",
    "- `src/`: Reusable Python modules for core logic (e.g., cleaning, outlier detection)\n",
    "- `scripts/`: Standalone Python scripts for pipeline stages (e.g., data acquisition, EDA)\n",
    "- `notebooks/`: Jupyter notebooks for exploratory analysis and prototyping\n",
    "- `models/`: Trained and serialized models (e.g., `model.pkl`)\n",
    "- `reports/`: Generated reports and figures\n",
    "- `app.py`: Flask application for model deployment\n",
    "\n",
    "## Quickstart\n",
    "1. `python -m venv .venv && source .venv/bin/activate`\n",
    "2. `pip install -r requirements.txt`\n",
    "3. `cp .env.example .env` (and optionally set `ALPHAVANTAGE_API_KEY`)\n",
    "4. `python main.py` to run the full pipeline.\n",
    "\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(ROOT / \"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"Written {ROOT / 'README.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481870d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71770a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c7bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12476af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c40667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a1d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd032700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da06a175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written src/storage.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "storage_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_format(path: str | Path) -> str:\n",
    "    ext = str(path).lower().rsplit('.', 1)[-1]\n",
    "    if ext in ('csv', 'parquet'):\n",
    "        return ext\n",
    "    raise ValueError(f'Unsupported file extension: {ext}')\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: str | Path) -> Path:\n",
    "    path = Path(path)\n",
    "    fmt = detect_format(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == 'csv':\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    elif fmt == 'parquet':\n",
    "        try:\n",
    "            df.to_parquet(path, index=False)\n",
    "        except Exception:\n",
    "            fallback = path.with_suffix('.csv')\n",
    "            df.to_csv(fallback, index=False)\n",
    "            return fallback\n",
    "        return path\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported format: {fmt}')\n",
    "\n",
    "def read_df(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    fmt = detect_format(path)\n",
    "    if fmt == 'csv':\n",
    "        df = pd.read_csv(path)\n",
    "        if 'date' in df.columns:\n",
    "            try:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return df\n",
    "    elif fmt == 'parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported format: {fmt}')\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"src/storage.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(storage_py)\n",
    "\n",
    "print(\"Written src/storage.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9225a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written src/cleaning.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "cleaning_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "def fill_missing_median(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns and pd.api.types.is_numeric_dtype(out[c]):\n",
    "            med = out[c].median()\n",
    "            out[c] = out[c].fillna(med)\n",
    "    return out\n",
    "\n",
    "def drop_missing(df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # drop columns above threshold missing\n",
    "    col_missing = out.isna().mean()\n",
    "    to_drop = [c for c, r in col_missing.items() if r > threshold]\n",
    "    if to_drop:\n",
    "        out = out.drop(columns=to_drop)\n",
    "    # drop remaining rows with any missing\n",
    "    out = out.dropna(axis=0, how='any')\n",
    "    return out\n",
    "\n",
    "def normalize_data(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns and pd.api.types.is_numeric_dtype(out[c]):\n",
    "            mu = out[c].mean()\n",
    "            sigma = out[c].std(ddof=0)\n",
    "            if sigma and not np.isnan(sigma) and sigma != 0:\n",
    "                out[c] = (out[c] - mu) / sigma\n",
    "    return out\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"src/cleaning.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaning_py)\n",
    "\n",
    "print(\"Written src/cleaning.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a48dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written src/outliers.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "outliers_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Iterable, Dict, Optional, Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OutlierMethod = Literal[\"iqr\", \"zscore\"]\n",
    "HandleMode = Literal[\"flag\", \"remove\", \"winsorize\", \"none\"]\n",
    "\n",
    "__all__ = [\n",
    "    \"detect_outliers_iqr\",\n",
    "    \"detect_outliers_zscore\",\n",
    "    \"winsorize_series\",\n",
    "    \"flag_outliers_df\",\n",
    "    \"remove_outliers_df\",\n",
    "    \"winsorize_df\",\n",
    "]\n",
    "\n",
    "def detect_outliers_iqr(series: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - k * iqr\n",
    "    upper = q3 + k * iqr\n",
    "    mask = (series < lower) | (series > upper)\n",
    "    return mask.fillna(False)\n",
    "\n",
    "def detect_outliers_zscore(series: pd.Series, threshold: float = 3.0) -> pd.Series:\n",
    "    mu = series.mean(skipna=True)\n",
    "    sigma = series.std(ddof=0, skipna=True)\n",
    "    if sigma == 0 or np.isnan(sigma):\n",
    "        return pd.Series(False, index=series.index)\n",
    "    z = (series - mu) / sigma\n",
    "    mask = z.abs() > threshold\n",
    "    return mask.fillna(False)\n",
    "\n",
    "def winsorize_series(series: pd.Series, lower: float = 0.05, upper: float = 0.95) -> pd.Series:\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    lo = series.quantile(lower)\n",
    "    hi = series.quantile(upper)\n",
    "    return series.clip(lower=lo, upper=hi)\n",
    "\n",
    "def flag_outliers_df(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Optional[Iterable[str]] = None,\n",
    "    method: OutlierMethod = \"iqr\",\n",
    "    method_params: Optional[Dict] = None,\n",
    "    flag_suffix: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    method_params = method_params or {}\n",
    "    out = df.copy()\n",
    "    for col in columns:\n",
    "        if not pd.api.types.is_numeric_dtype(out[col]):\n",
    "            continue\n",
    "        if method == \"iqr\":\n",
    "            mask = detect_outliers_iqr(out[col], **method_params)\n",
    "            suffix = flag_suffix or \"outlier_iqr\"\n",
    "        elif method == \"zscore\":\n",
    "            mask = detect_outliers_zscore(out[col], **method_params)\n",
    "            suffix = flag_suffix or \"outlier_z\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        out[f\"{col}_{suffix}\"] = mask\n",
    "    return out\n",
    "\n",
    "def remove_outliers_df(\n",
    "    df: pd.DataFrame,\n",
    "    flag_columns: Optional[Iterable[str]] = None,\n",
    "    how: Literal[\"any\", \"all\"] = \"any\",\n",
    ") -> pd.DataFrame:\n",
    "    if flag_columns is None:\n",
    "        flag_columns = [c for c in df.columns if \"outlier\" in c]\n",
    "    if not flag_columns:\n",
    "        return df.copy()\n",
    "    mask = df[flag_columns].any(axis=1) if how == \"any\" else df[flag_columns].all(axis=1)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def winsorize_df(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Optional[Iterable[str]] = None,\n",
    "    lower: float = 0.05,\n",
    "    upper: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    out = df.copy()\n",
    "    for col in columns:\n",
    "        if pd.api.types.is_numeric_dtype(out[col]):\n",
    "            out[col] = winsorize_series(out[col], lower=lower, upper=upper)\n",
    "    return out\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"src/outliers.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(outliers_py)\n",
    "\n",
    "print(\"Written src/outliers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a20ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written scripts/acquire.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "acquire_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pathlib\n",
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "RAW = pathlib.Path('data/raw')\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "load_dotenv()\n",
    "\n",
    "def ts() -> str:\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta) -> pathlib.Path:\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k, v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"Saved\", path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}\n",
    "\n",
    "def acquire_api(symbol: str = 'AAPL') -> pd.DataFrame:\n",
    "    use_alpha = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "    if use_alpha:\n",
    "        url = 'https://www.alphavantage.co/query'\n",
    "        params = {'function':'TIME_SERIES_DAILY','symbol':symbol,'outputsize':'full','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        key = [k for k in js if 'Time Series' in k][0]\n",
    "        df_api = pd.DataFrame(js[key]).T\n",
    "        df_api.columns = [c.split('. ')[1] for c in df_api.columns]\n",
    "        df_api = df_api.reset_index().rename(columns={'index':'date'})\n",
    "        df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "        for col in ['open','high','low','close','volume']:\n",
    "            df_api[col] = pd.to_numeric(df_api[col], errors='coerce')\n",
    "    else:\n",
    "        import yfinance as yf\n",
    "        df_api = yf.download(symbol, period='3mo', interval='1d').reset_index()\n",
    "        df_api = df_api.rename(columns={'Date':'date','Open':'open','High':'high','Low':'low','Close':'close','Volume':'volume'})\n",
    "    required_cols = ['date','open','high','low','close','volume']\n",
    "    v = validate(df_api, required_cols)\n",
    "    print(\"API Validation Results:\", v)\n",
    "    return df_api\n",
    "\n",
    "def acquire_scrape(url: str = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') -> pd.DataFrame:\n",
    "    headers = {'User-Agent':'AFE-Project/1.0'}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "        header, *data = [r for r in rows if r]\n",
    "        df_scrape = pd.DataFrame(data, columns=header)\n",
    "    except Exception as e:\n",
    "        print('Scrape failed, using inline demo table:', e)\n",
    "        html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "        header, *data = [r for r in rows if r]\n",
    "        df_scrape = pd.DataFrame(data, columns=header)\n",
    "    if 'Price' in df_scrape.columns:\n",
    "        df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
    "    required_cols = ['Symbol','Security']\n",
    "    v = {'missing':[c for c in required_cols if c not in df_scrape.columns], 'shape': df_scrape.shape, 'na_total': int(df_scrape.isna().sum().sum())}\n",
    "    print(\"Scrape Validation Results:\", v)\n",
    "    return df_scrape\n",
    "\n",
    "def main():\n",
    "    df_api = acquire_api('AAPL')\n",
    "    _ = save_csv(df_api.sort_values('date'), prefix='api', source='alpha' if bool(os.getenv('ALPHAVANTAGE_API_KEY')) else 'yfinance', symbol='AAPL')\n",
    "\n",
    "    df_scrape = acquire_scrape()\n",
    "    _ = save_csv(df_scrape, prefix='scrape', site='wikipedia', table='SP500-List')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"scripts/acquire.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(acquire_py)\n",
    "\n",
    "print(\"Written scripts/acquire.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d838ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written scripts/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "preprocess_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.outliers import flag_outliers_df, remove_outliers_df, winsorize_df\n",
    "\n",
    "def ensure_dataset(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    # Fallback synthetic linear dataset with extremes\n",
    "    x = np.linspace(0, 10, 200)\n",
    "    y = 2.2 * x + 1 + np.random.normal(0, 1.2, size=x.size)\n",
    "    y[10] += 15; y[120] -= 13; y[160] += 18\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    raw_path = Path(\"data/raw/outliers_homework.csv\")\n",
    "    df = ensure_dataset(raw_path)\n",
    "\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    if not num_cols:\n",
    "        raise ValueError(\"No numeric columns found for outlier processing.\")\n",
    "    target_col = 'y' if 'y' in df.columns else num_cols[0]\n",
    "\n",
    "    df_flagged = flag_outliers_df(df, columns=[target_col], method=\"iqr\", method_params={\"k\": 1.5})\n",
    "\n",
    "    df_removed = remove_outliers_df(df_flagged, flag_columns=[f\"{target_col}_outlier_iqr\"], how=\"any\")\n",
    "    df_wins = winsorize_df(df_flagged, columns=[target_col], lower=0.05, upper=0.95)\n",
    "\n",
    "    Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_flagged.to_csv(\"data/interim/with_outlier_flags.csv\", index=False)\n",
    "    df_removed.to_csv(\"data/processed/removed_outliers.csv\", index=False)\n",
    "    df_wins.to_csv(\"data/processed/winsorized.csv\", index=False)\n",
    "\n",
    "    print(\"Wrote data/interim/with_outlier_flags.csv\")\n",
    "    print(\"Wrote data/processed/removed_outliers.csv\")\n",
    "    print(\"Wrote data/processed/winsorized.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"scripts/preprocess.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(preprocess_py)\n",
    "\n",
    "print(\"Written scripts/preprocess.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507d6739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written scripts/sensitivity.py\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "sensitivity_py = dedent(\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.outliers import flag_outliers_df, winsorize_series\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "def ensure_dataset(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    x = np.linspace(0, 10, 200)\n",
    "    y = 2.2 * x + 1 + np.random.normal(0, 1.2, size=x.size)\n",
    "    y[10] += 15; y[120] -= 13; y[160] += 18\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    data_path = Path('data/raw/outliers_homework.csv')\n",
    "    df = ensure_dataset(data_path)\n",
    "    target_col = 'y' if 'y' in df.columns else df.select_dtypes(include='number').columns[0]\n",
    "\n",
    "    df = flag_outliers_df(df, columns=[target_col], method=\"iqr\", method_params={\"k\": 1.5})\n",
    "    df = flag_outliers_df(df, columns=[target_col], method=\"zscore\", method_params={\"threshold\": 3.0})\n",
    "\n",
    "    pct_iqr = df[f'{target_col}_outlier_iqr'].mean() * 100\n",
    "    pct_z = df[f'{target_col}_outlier_z'].mean() * 100\n",
    "    print(f\"Flagged (%): IQR={pct_iqr:.2f}%, Z={pct_z:.2f}%\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.boxplot(df[target_col].dropna())\n",
    "    plt.title(f'Boxplot: {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/interim/boxplot.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(df[target_col].dropna(), bins=30)\n",
    "    plt.title(f'Histogram: {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/interim/hist.png')\n",
    "    plt.close()\n",
    "\n",
    "    summ_all = df[target_col].describe()[['mean', '50%', 'std']].rename({'50%': 'median'})\n",
    "    summ_filtered = df.loc[~df[f'{target_col}_outlier_iqr'], target_col].describe()[['mean', '50%', 'std']].rename({'50%': 'median'})\n",
    "    w = winsorize_series(df[target_col], lower=0.05, upper=0.95)\n",
    "    summ_w = w.describe()[['mean', '50%', 'std']].rename({'50%': 'median'})\n",
    "\n",
    "    comp = pd.concat({'all': summ_all, 'filtered_iqr': summ_filtered, 'winsorized': summ_w}, axis=1)\n",
    "    comp.to_csv('data/interim/sensitivity_summary.csv')\n",
    "    print(\"Wrote data/interim/sensitivity_summary.csv\")\n",
    "\n",
    "    if 'x' in df.columns:\n",
    "        X_all = df[['x']].to_numpy(); y_all = df[target_col].to_numpy()\n",
    "        X_flt = df.loc[~df[f'{target_col}_outlier_iqr'], ['x']].to_numpy()\n",
    "        y_flt = df.loc[~df[f'{target_col}_outlier_iqr'], target_col].to_numpy()\n",
    "\n",
    "        model_all = LinearRegression().fit(X_all, y_all)\n",
    "        model_flt = LinearRegression().fit(X_flt, y_flt)\n",
    "\n",
    "        mae_all = mean_absolute_error(y_all, model_all.predict(X_all))\n",
    "        mae_flt = mean_absolute_error(y_flt, model_flt.predict(X_flt))\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            'slope': [model_all.coef_[0], model_flt.coef_[0]],\n",
    "            'intercept': [model_all.intercept_, model_flt.intercept_],\n",
    "            'r2': [model_all.score(X_all, y_all), model_flt.score(X_flt, y_flt)],\n",
    "            'mae': [mae_all, mae_flt]\n",
    "        }, index=['all', 'filtered_iqr'])\n",
    "        results.to_csv('data/interim/regression_comparison.csv')\n",
    "        print(\"Wrote data/interim/regression_comparison.csv\")\n",
    "    else:\n",
    "        print(\"No 'x' column; skipped regression comparison.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"scripts/sensitivity.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sensitivity_py)\n",
    "\n",
    "print(\"Written scripts/sensitivity.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779aef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written docs/outliers.md\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Write docs/outliers.md\n",
    "from textwrap import dedent\n",
    "outliers_md = dedent(\"\"\"\n",
    "# Outlier Handling: Definition, Methods, Assumptions, and Risks  \n",
    "\n",
    "## Definition  \n",
    "Observations that significantly deviate from the overall distribution pattern are considered outliers; detection methods include IQR and Z-score.  \n",
    "\n",
    "## Methods and Thresholds  \n",
    "- **IQR**: k=1.5, using quartiles and the interquartile range to identify outliers.  \n",
    "- **Z-score**: Threshold of 3.0, assumes approximate normality, more sensitive to heavy-tailed distributions.  \n",
    "- **Winsorize**: Trimming at the 5th and 95th percentiles to reduce the impact of extreme values without deleting data.  \n",
    "\n",
    "## Assumptions  \n",
    "- Extreme observations are mostly noise or abnormal processes rather than genuine business events.  \n",
    "- Missing values are excluded from statistical calculations; constant columns are not flagged as outliers (no variance).  \n",
    "\n",
    "## Sensitivity Analysis Design  \n",
    "- Compare mean/median/standard deviation between \"original data,\" \"IQR outlier removal,\" and \"Winsorize.\"  \n",
    "- If an independent variable x exists, perform simple bivariate regression to compare slope/intercept/R²/MAE.  \n",
    "\n",
    "## Observations and Impact (Update based on actual results)  \n",
    "- After removing outliers, MAE generally decreases while R² increases; the slope aligns more closely with the main trend (subject to actual output).  \n",
    "\n",
    "## Risks  \n",
    "- Risk of mistakenly removing genuine extreme events (e.g., rare but significant peaks).  \n",
    "- Methods may misjudge in asymmetric or multimodal distributions; Z-score is less robust for heavy-tailed distributions.  \n",
    "- Over-cleaning reduces sample size and increases model variance.  \n",
    "\n",
    "## Mitigation Strategies  \n",
    "- Prioritize Winsorizing or merely flagging key business metrics.  \n",
    "- Record thresholds, deletion ratios, and changes in performance metrics; conduct business reviews if necessary.  \n",
    "- Use robust loss functions or robust regression in downstream tasks.  \n",
    "\n",
    "## Reproducing the Experiment  \n",
    "- Data: `data/raw/outliers_homework.csv` (if missing, synthetic data will be automatically generated).  \n",
    "- Scripts: `scripts/preprocess.py`, `scripts/sensitivity.py`.  \n",
    "- Output: Comparison files and plots in `data/interim` and `data/processed`.\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"docs/outliers.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(outliers_md)\n",
    "\n",
    "print(\"Written docs/outliers.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb2b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written requirements.txt\n"
     ]
    }
   ],
   "source": [
    "reqs = \"\"\"\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "matplotlib\n",
    "requests\n",
    "beautifulsoup4\n",
    "python-dotenv\n",
    "yfinance\n",
    "pyarrow\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reqs.strip() + \"\\n\")\n",
    "print(\"Written requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486df2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written .gitignore\n"
     ]
    }
   ],
   "source": [
    "gitignore = \"\"\"\n",
    ".env\n",
    "__pycache__/\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "*.pyc\n",
    "data/raw/*.csv\n",
    "data/interim/*\n",
    "data/processed/*\n",
    "\"\"\"\n",
    "with open(\".gitignore\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(gitignore.lstrip())\n",
    "print(\"Written .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f3c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written .env.example\n"
     ]
    }
   ],
   "source": [
    "env_example = \"ALPHAVANTAGE_API_KEY=NOL8361L3I5LIPQX\\n\"\n",
    "with open(\".env.example\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(env_example)\n",
    "print(\"Written .env.example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "571ac17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written README.md\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "readme = dedent(\"\"\"\n",
    "# Outlier Analysis Project\n",
    "\n",
    "## Overview\n",
    "A small end-to-end pipeline integrating data acquisition, storage, preprocessing, and explicit outlier management with sensitivity analysis.\n",
    "\n",
    "## Structure\n",
    "- data/{raw,interim,processed}\n",
    "- src/{config.py,storage.py,cleaning.py,outliers.py}\n",
    "- scripts/{acquire.py,preprocess.py,sensitivity.py}\n",
    "- docs/outliers.md\n",
    "- notebooks/ (optional for visual analysis)\n",
    "\n",
    "## Quickstart\n",
    "1) python -m venv .venv && source .venv/bin/activate\n",
    "2) pip install -r requirements.txt\n",
    "3) cp .env.example .env  # optionally set ALPHAVANTAGE_API_KEY\n",
    "4) python scripts/acquire.py  # optional: pull sample API/scrape data\n",
    "5) python scripts/preprocess.py  # outlier flag/remove/winsorize\n",
    "6) python scripts/sensitivity.py  # generate summary and plots\n",
    "\n",
    "## Notes\n",
    "- If data/raw/outliers_homework.csv doesn't exist, scripts will generate a synthetic dataset.\n",
    "- Parquet writing gracefully falls back to CSV if engine is unavailable.\n",
    "\"\"\").lstrip()\n",
    "\n",
    "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Written README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
