# Applied Financial Engineering — Framework Guide Template

This Framework Guide is a structured reflection tool.  
Fill it in as you progress through the course or at the end of your project.  
It will help you connect each stage of the **Applied Financial Engineering Lifecycle** to your own project, and create a portfolio-ready artifact.

---

## How to Use
- Each row corresponds to one stage in the lifecycle.  
- Use the prompts to guide your answers.  
- Be concise but specific — 2–4 sentences per cell is often enough.  
- This is **not a test prep sheet**. It’s a way to *document, reflect, and demonstrate* your process.

---

## Framework Guide Table

| Lifecycle Stage | What You Did | Challenges | Solutions / Decisions | Future Improvements |
|---|---|---|---|---|
| **1. Problem Framing & Scoping** | Defined the problem as predicting a continuous target based on two numerical features. The goal was to build a simple, interpretable baseline model and deploy it as a REST API. | The initial scope was broad. A key challenge was narrowing it down to a manageable baseline (Linear Regression) instead of trying to build a complex model from the start. | We decided that a functional, end-to-end API serving a simple model was the primary success metric, prioritizing the full lifecycle over model complexity. | Next time, I would create a more detailed project brief upfront, explicitly defining "out-of-scope" items to prevent scope creep. |
| **2. Tooling Setup** | Set up a Python environment using `pip` and managed dependencies in a `requirements.txt` file. Key libraries included scikit-learn, Flask, Pandas, and Streamlit. | Ensuring library versions were consistent between the notebook, API script, and dashboard script was a minor challenge that could lead to runtime errors. | We created a single, unified `requirements.txt` file for the entire project and used it to create a clean virtual environment from scratch to ensure reproducibility. | Implement containerization using Docker to create a completely isolated and portable environment, eliminating any local dependency issues. |
| **3. Python Fundamentals** | Applied core Python skills for scripting (Flask app), data manipulation (Pandas DataFrames), and creating modular code (functions in `src/utils.py`). | Initially, all logic was in the notebook. The main challenge was identifying which parts were truly "reusable" and refactoring them into clean functions. | We adopted a rule: if a piece of code is used more than once or performs a single, well-defined task (e.g., data cleaning), it should be a function. | Continue to practice writing more Pythonic code, such as using list comprehensions and lambda functions to make code more concise. |
| **4. Data Acquisition / Ingestion** | Ingested data from a mock CSV file. The process was designed to simulate reading from a static, local data source. | The provided data was synthetic and clean. In a real project, accessing and parsing data from a live API or database would be a major challenge. | We created a dedicated `ingest_data` task in our orchestration plan to isolate this step, making it easy to swap out the data source later. | Improve the ingestion script to handle API authentication, rate limiting, and potential network errors with a retry mechanism. |
| **5. Data Storage** | Stored data in version-controlled directories: `data/raw`, `data/processed`, and `data/features`. Checkpoints were saved as CSV files. | Working with flat files (CSVs) is simple but doesn't scale well and lacks features like transactional integrity or schema enforcement. | For this project's scope, CSVs were sufficient. We used a clear naming convention and folder structure to maintain order. | For a larger project, I would use a more robust storage solution like a Parquet data lake or a SQL database (e.g., PostgreSQL). |
| **6. Data Preprocessing** | Performed basic data cleaning by identifying and dropping rows with null values using `pandas.dropna()`. | The main challenge was deciding on a simple, robust strategy. While dropping nulls is easy, it can lead to data loss if nulls are frequent. | We decided dropping nulls was an acceptable baseline strategy for the initial model, as creating a complex imputation method was out of scope. | Implement more sophisticated imputation strategies (e.g., mean, median, or model-based imputation) and log the number of rows affected. |
| **7. Outlier Analysis** | Conducted a sensitivity analysis (Stage 12) to understand the impact of outlier removal rules (e.g., 3σ rule) on model performance. | It was challenging to determine if an outlier was an error or a real but infrequent event without deeper domain knowledge. | For the baseline, we kept the outliers but noted in our report that an outlier removal strategy could significantly boost returns, flagging it as a key risk. | Use more advanced outlier detection techniques like Isolation Forest or DBSCAN and create visualizations to help a human expert review them. |
| **8. Exploratory Data Analysis (EDA)** | Used `seaborn` and `matplotlib` to create visualizations like scatter plots, bar charts, and line charts to understand relationships and distributions in the data. | With only two features, EDA was straightforward. A key challenge in a real project would be navigating a high-dimensional feature space. | We focused on creating stakeholder-ready charts with clear titles and labels, which directly fed into our final report (Stage 12). | I would add automated EDA reports using a library like `ydata-profiling` to speed up the initial analysis phase. |
| **9. Feature Engineering** | The project used raw features directly, so no complex feature engineering was performed. This was a deliberate choice to keep the baseline model simple. | The challenge was resisting the urge to create complex features prematurely. It's easy to spend too much time here without a clear signal of improvement. | We decided to establish a strong baseline with raw features first. This provides a benchmark against which all future feature engineering efforts can be measured. | Add domain-specific features. For a financial model, this could include moving averages, volatility measures, or interaction terms. |
| **10. Modeling** | Trained a `LinearRegression` model using scikit-learn. The model was chosen for its simplicity and interpretability. | The model itself was simple to train. The challenge was in the "last mile" – ensuring the pickled model object could be correctly loaded and used by the Flask API. | We created a dedicated notebook cell to test the pickle/unpickle process and a sample prediction, which caught issues before moving to the API script. | I would experiment with tree-based models (like XGBoost) to capture non-linearities and compare their performance against the linear baseline. |
| **11. Evaluation & Risk Communication** | Evaluated the model using Mean Absolute Error (MAE) and R-squared. We communicated risks through a sensitivity analysis table. | A key challenge was translating statistical metrics (like MAE) into tangible business impact and risk for stakeholders. | We used plain-language takeaways and a "Decision Implications" section in our report to bridge the gap between technical results and business actions. | I would add backtesting with walk-forward validation to provide a more realistic estimate of the model's out-of-sample performance. |
| **12. Results Reporting** | Produced a final report in a notebook format, including an executive summary, annotated charts, and clear recommendations. | The main challenge was tailoring the level of technical detail for a non-technical audience without oversimplifying the findings. | We adopted a layered approach: a high-level executive summary, visual charts with one-sentence takeaways, and detailed appendices for technical reviewers. | Create two separate deliverables: a concise slide deck for executive review and a detailed technical report (or notebook) for the data science team. |
| **13. Productization** | Prepared the model for use by saving it as a `model.pkl` file. We also created a Flask API with a `/predict` endpoint to serve the model. | Ensuring the API's input format matched the model's expected input (a 2D NumPy array) was a small but critical challenge. | We added input validation and clear error handling to the Flask app to make it more robust against incorrect requests. | I would containerize the Flask application using Docker to ensure a consistent and isolated runtime environment for deployment. |
| **14. Deployment & Monitoring** | Designed a monitoring plan covering four layers (Data, Model, System, Business) with specific metrics and alert thresholds. | The biggest challenge was thinking beyond model accuracy and considering real-world operational issues like data freshness and API latency. | We created a detailed monitoring table that specified metrics (e.g., p95 latency > 250ms), thresholds, and owners for each potential issue. | I would implement a logging service (like ELK stack or Datadog) to centralize logs from the pipeline and API for easier debugging and monitoring. |
| **15. Orchestration & System Design**| Decomposed the project into a Directed Acyclic Graph (DAG) of tasks (ingest, clean, train, etc.) and defined their dependencies. | The main challenge was thinking about the project as a sequence of independent, repeatable tasks rather than a single, monolithic notebook. | We created a clear orchestration plan with defined inputs, outputs, and checkpoints for each task, making the process ready for an orchestration tool. | I would implement the DAG using a lightweight orchestrator like Prefect or Dagster to automate the entire workflow from data ingestion to model training. |
| **16. Lifecycle Review & Reflection** | This guide serves as the primary reflection, mapping each course stage to our project and documenting key decisions and learnings. | The biggest challenge was retrospectively fitting the project into the framework, as some steps were done implicitly rather than explicitly. | This structured table forced a clear documentation of the end-to-end process, highlighting strengths and weaknesses. | In the next project, I will use this framework from the beginning as a living document to guide and document decisions at each stage. |


