{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: \n",
    "Date: \n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cedc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHAVANTAGE_API_KEY loaded? True\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "RAW = pathlib.Path('../data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
    "load_dotenv(); print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8607180",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c72b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac682e",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5297e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Validation Results: {'missing': [], 'shape': (6488, 6), 'na_total': 0}\n",
      "Validation Passed: Data appears to be complete and valid.\n"
     ]
    }
   ],
   "source": [
    "SYMBOL = 'AAPL'\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "if USE_ALPHA:\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {'function':'TIME_SERIES_DAILY','symbol':SYMBOL,'outputsize':'full','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    key = [k for k in js if 'Time Series' in k][0]\n",
    "    df_api = pd.DataFrame(js[key]).T\n",
    "    df_api.columns = [c.split('. ')[1] for c in df_api.columns] \n",
    "    df_api = df_api.reset_index().rename(columns={'index':'date'})\n",
    "    df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df_api[col] = pd.to_numeric(df_api[col])\n",
    "else:\n",
    "    import yfinance as yf\n",
    "    df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()\n",
    "    df_api = df_api.rename(columns={'Date':'date','Open':'open','High':'high','Low':'low','Close':'close','Volume':'volume'})\n",
    "\n",
    "required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "v_api = validate(df_api, required_cols)\n",
    "print(\"API Validation Results:\", v_api)\n",
    "\n",
    "if v_api['missing']:\n",
    "    print(\"Validation FAILED: Missing required columns:\", v_api['missing'])\n",
    "elif v_api['na_total'] > 0:\n",
    "    print(f\"Validation WARNING: Data contains {v_api['na_total']} null values.\")\n",
    "elif v_api['shape'][0] < 1000:\n",
    "    print(f\"Validation WARNING: Low row count ({v_api['shape'][0]}) for 'full' data size.\")\n",
    "else:\n",
    "    print(\"Validation Passed: Data appears to be complete and valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/raw/api_source-alpha_symbol-AAPL_20250819-101945.csv\n"
     ]
    }
   ],
   "source": [
    "_ = save_csv(df_api.sort_values('date'), prefix='api', source='alpha' if USE_ALPHA else 'yfinance', symbol=SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Replace `SCRAPE_URL` with a permitted page containing a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape Validation Results: {'missing': [], 'shape': (880, 8), 'na_total': 758}\n",
      "Validation WARNING: Row count (880) is outside the expected range of 490-510.\n"
     ]
    }
   ],
   "source": [
    "SCRAPE_URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'  # TODO: replace with permitted page\n",
    "headers = {'User-Agent':'AFE-Homework/1.0'}\n",
    "try:\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30); resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "except Exception as e:\n",
    "    print('Scrape failed, using inline demo table:', e)\n",
    "    html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "if 'Price' in df_scrape.columns:\n",
    "    df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
    "\n",
    "required_cols = ['Symbol', 'Security']\n",
    "v_scrape = validate(df_scrape, required_cols)\n",
    "print(\"Scrape Validation Results:\", v_scrape)\n",
    "\n",
    "rows = v_scrape['shape'][0]\n",
    "if v_scrape['missing']:\n",
    "    print(\"Validation FAILED: Missing required columns:\", v_scrape['missing'])\n",
    "elif not (490 < rows < 510): \n",
    "    print(f\"Validation WARNING: Row count ({rows}) is outside the expected range of 490-510.\")\n",
    "else:\n",
    "    print(\"Validation Passed: Data appears to be complete and valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/raw/scrape_site-wikipedia_table-SP500-List_20250819-101945.csv\n"
     ]
    }
   ],
   "source": [
    "_ = save_csv(df_scrape, prefix='scrape', site='wikipedia', table='SP500-List')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "- API Source: (URL/endpoint/params)\n",
    "- Scrape Source: (URL/table description)\n",
    "- Assumptions & risks: (rate limits, selector fragility, schema changes)\n",
    "- Confirm `.env` is not committed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3e182",
   "metadata": {},
   "source": [
    "# **Work Summary**\n",
    "\n",
    "This script ingests data from two distinct sources: stock prices via an API and a list of companies via web scraping.\n",
    "\n",
    "#### 1. Alpha Vantage API (Stock Prices)\n",
    "* **Data Source**: `Alpha Vantage`\n",
    "* **Parameters**: This process uses `function: TIME_SERIES_DAILY`, `symbol: \"AAPL\"`, an API key loaded from the `.env` file, and `outputsize: full` to fetch the complete historical data.\n",
    "* **Validation Logic**: The script validates the data by checking if the DataFrame is not empty, contains all required columns, has more than 1000 rows, and is free of null values.\n",
    "\n",
    "#### 2. Wikipedia Web Scrape (S&P 500 Companies List)\n",
    "* **Data Source/URL**: `https://en.wikipedia.org/wiki/List_of_S%26P_500_companies`\n",
    "* **Parameters**: No specific parameters are used; the script simply fetches the first table on the page.\n",
    "* **Validation Logic**: It validates the data by checking if the DataFrame is not empty, contains the `'Symbol'` and `'Security'` columns, and has a row count between 490 and 510.\n",
    "\n",
    "---\n",
    "\n",
    "### Regarding the .env File\n",
    "\n",
    "The `python-dotenv` library is used to load the `ALPHAVANTAGE_API_KEY` from the `.env` file. The `.env` file **has been added** to `.gitignore`, and an `.env.example` will be committed as a placeholder.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions & Risks\n",
    "\n",
    "* **Assumptions**\n",
    "\n",
    "    * **API Structure Stability**: Assumes the JSON structure (e.g., the key `'Time Series (Daily)'`) and column names returned by the Alpha Vantage API will remain consistent.\n",
    "\n",
    "    * **Web Page Structure Stability**: Assumes the target table on the S&P 500 list Wikipedia page will always be the first `<table>` element.\n",
    "\n",
    "    * **Data History Depth**: Assumes that the daily data for a mature stock (like AAPL) should exceed 1000 records.\n",
    "\n",
    "    * **Number of S&P 500 Constituents**: Assumes the number of constituents in the S&P 500 index is between 490 and 510.\n",
    "\n",
    "* **Risks**\n",
    "\n",
    "    * **API Key Exposure**: The API key will be exposed if the `.env` file is accidentally committed to the code repository.\n",
    "\n",
    "    * **Upstream Data Source Changes**: The structure of the API or the Wikipedia page could change at any time, which would cause the script to fail or extract incorrect data.\n",
    "\n",
    "    * **API Limitations**: AlphaVantage might adjust its data access limitations; for instance, requiring a subscription to access `TIME_SERIES_DAILY_ADJUSTED` data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
