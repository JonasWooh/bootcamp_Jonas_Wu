{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 14: Deployment & Monitoring\n",
    "\n",
    "Use this template to draft your reflection and (optionally) sketch a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reflection (200–300 words)\n",
    "- Risks if deployed:\n",
    "- Monitoring metrics across layers (Data/Model/System/Business):\n",
    "- Ownership & handoffs:\n",
    "\n",
    "> Tip: Be specific (e.g., 'p95 latency > 250ms triggers on-call notification')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868865f",
   "metadata": {},
   "source": [
    "### Deployment Risks:\n",
    "If deployed, our predictive model faces several risks. First, **Data Drift** could occur if the statistical properties of incoming features (e.g., market volatility, economic indicators) change over time, making our model's assumptions invalid. Second, **Data Quality Degradation** is a significant threat; an increase in null values or a schema change from an upstream data source could cause prediction failures. Lastly, **Concept Drift**, where the relationship between features and the target variable changes (e.g., due to a new market regime), could silently degrade model performance over time.\n",
    "\n",
    "### Monitoring Metrics Across Layers:\n",
    "To mitigate these risks, we would implement a multi-layered monitoring strategy:\n",
    "* **Data Layer**: We will monitor the **null rate** for key input features, alerting the Data Engineering on-call team if it exceeds 5%. Data **freshness** will also be tracked, with an alert if the latest data batch is more than 60 minutes old.\n",
    "* **Model Layer**: A **rolling 7-day Mean Absolute Error (MAE)** will be our primary performance metric. If the MAE increases by 15% above the baseline established during training, the Data Science team will be notified to investigate. We will trigger a model retraining pipeline if the Population Stability Index (PSI) on key features exceeds 0.1, indicating significant data drift.\n",
    "* **System Layer**: We will monitor the API's **p95 prediction latency**, with a threshold of 250ms triggering an alert to the Platform on-call team. We will also track the **server error rate**, aiming to keep it below 0.1%.\n",
    "* **Business Layer**: We will monitor a key business KPI, such as the **rate of successful trades based on model signals**. A sustained dip of 10% below the quarterly average would trigger a weekly review by the Business Analyst team.\n",
    "\n",
    "### Ownership & Handoffs:\n",
    "Clear ownership is crucial for ongoing maintenance. The **Data Science team** owns the model logic, performance monitoring dashboards, and the retraining process. The **Platform team** owns the API infrastructure, including system-level monitoring (latency, errors) and approves any rollbacks. The **Data Engineering team** is responsible for the upstream data pipelines and data quality alerts. Handoffs will be managed via a shared Jira board where issues are logged, assigned, and tracked through resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Optional: Dashboard Sketch\n",
    "Describe panels and key charts. You can also attach an image file in your repo (png/pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['freshness_minutes', 'null_rate', 'schema_hash'],\n",
       " 'model': ['rolling_mae_or_auc', 'calibration_error'],\n",
       " 'system': ['p95_latency_ms', 'error_rate'],\n",
       " 'business': ['approval_rate', 'bad_rate']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional helper: simple structure to list metrics\n",
    "monitoring = {\n",
    "    'data': ['freshness_minutes', 'null_rate', 'schema_hash'],\n",
    "    'model': ['rolling_mae_or_auc', 'calibration_error'],\n",
    "    'system': ['p95_latency_ms', 'error_rate'],\n",
    "    'business': ['approval_rate', 'bad_rate']\n",
    "}\n",
    "monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d12cd",
   "metadata": {},
   "source": [
    "### Monitoring Dashboard Sketch\n",
    "\n",
    "**Title: Model Performance & Health Dashboard**\n",
    "\n",
    "* **Panel 1: Key Performance Indicators (KPIs)**\n",
    "    * **Metric:** Current 7-Day Rolling MAE (big number display with trend arrow).\n",
    "    * **Metric:** Daily Prediction Volume.\n",
    "    * **Metric:** Business KPI: Successful Trade Rate (%).\n",
    "\n",
    "* **Panel 2: Model Performance Over Time**\n",
    "    * **Chart:** Line chart showing Rolling MAE vs. the 15% alert threshold over the last 30 days.\n",
    "    * **Chart:** Line chart showing the model's calibration error over time.\n",
    "\n",
    "* **Panel 3: Data Drift & Quality**\n",
    "    * **Chart:** Bar chart showing the Population Stability Index (PSI) for the top 5 features (updated daily).\n",
    "    * **Metric:** Null Rate % for critical features.\n",
    "    * **Metric:** Data Freshness (in minutes).\n",
    "\n",
    "* **Panel 4: System Health**\n",
    "    * **Chart:** Line chart of p95 Latency (ms) over the last 24 hours with the 250ms alert threshold shown.\n",
    "    * **Metric:** API Error Rate (%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dd319",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
